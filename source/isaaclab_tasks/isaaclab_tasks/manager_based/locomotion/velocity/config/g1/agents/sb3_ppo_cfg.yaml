# from isaaclab.utils import configclass
# from stable_baselines3.common.callbacks import CheckpointCallback

# @configclass
# class G1RoughSB3PPOCfg:
#     """SB3 PPO configuration for G1 robot on rough terrain."""
    
#     # 基本參數
#     algo_name = "PPO"
#     policy_type = "MlpPolicy"
#     device = "cuda"
#     seed = 0
    
#     # PPO 超參數
#     learning_rate = 3e-4
#     n_steps = 2048
#     batch_size = 64
#     n_epochs = 10
#     gamma = 0.99
#     gae_lambda = 0.95
#     clip_range = 0.2
#     ent_coef = 0.0
#     vf_coef = 0.5
#     max_grad_norm = 0.5
    
#     # 訓練配置
#     total_timesteps = 10000000
#     log_interval = 1
    
#     # 回調函數
#     checkpoint_callback = CheckpointCallback(
#         save_freq=100000,
#         save_path="./logs/sb3/",
#         name_prefix="g1_rough",
#         save_replay_buffer=False,
#         save_vecnormalize=True,
#     )
# @configclass
# class G1FlatSB3PPOCfg:
#     """SB3 PPO configuration for G1 robot on flat terrain."""
    
#     # 基本參數
#     algo_name = "PPO"
#     policy_type = "MlpPolicy"
#     device = "cuda"
#     seed = 0
    
#     # PPO 超參數 (可能與粗糙地形略有不同)
#     learning_rate = 3e-4
#     n_steps = 2048
#     batch_size = 64
#     n_epochs = 10
#     gamma = 0.99
#     gae_lambda = 0.95
#     clip_range = 0.2
#     ent_coef = 0.0
#     vf_coef = 0.5
#     max_grad_norm = 0.5
    
#     # 訓練配置
#     total_timesteps = 5000000  # 平坦地形可能需要較少的時間步
#     log_interval = 1
    
#     # 回調函數
#     checkpoint_callback = CheckpointCallback(
#         save_freq=100000,
#         save_path="./logs/sb3/",
#         name_prefix="g1_flat",
#         save_replay_buffer=False,
#         save_vecnormalize=True,
#     )

# Reference: https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/ppo.yml#L245
# seed: 42

# policy: 'MlpPolicy'
# n_timesteps: !!float 5e7
# # batch_size: 256
# n_steps: 512
# gamma: 0.99
# learning_rate: !!float 2.5e-4
# ent_coef: 0.0
# # clip_range: 0.2
# # n_epochs: 10
# gae_lambda: 0.95
# max_grad_norm: 1.0
# vf_coef: 0.5
# device: "cuda:0"
# policy_kwargs: "dict(
#                   log_std_init=-1,
#                   ortho_init=False,
#                   activation_fn=nn.ReLU,
#                   net_arch=dict(pi=[256, 256], vf=[256, 256])
#                 )"

# seed: 42

# policy: 'MlpPolicy'
# n_steps: 512
# batch_size: 256
# train_freq: 1  # 每次環境步數後進行一次訓練
# gradient_steps: 1  # 每次更新的梯度步數
# learning_rate: !!float 3e-4
# gamma: 0.99
# tau: 0.005  # 目標網路更新的平滑係數
# ent_coef: "auto"  # 熵正則化係數，設為 "auto" 以自動調整
# target_entropy: "auto"  # 目標熵值，默認為自動計算
# buffer_size: 1_000_000  # 經驗回放緩衝區大小
# device: "cuda:0"
# policy_kwargs: "dict(
#                   log_std_init=-3,
#                   net_arch=dict(pi=[256, 256], qf=[256, 256]),
#                   activation_fn=nn.ReLU
#                 )"

# # ---------------------------------------SAC---------------------------------------
seed: 50

policy: 'MlpPolicy'
n_steps: 512
batch_size: 256 ############
n_timesteps: !!float 5e6  # 總訓練時間步數

learning_starts: 10000 ############
# train_freq: 1  # 每次環境步數後進行一次訓練
train_freq: 1  ############
gradient_steps: 1  # 每次更新的梯度步數
learning_rate: !!float 3e-4
gamma: 0.99
# target_entropy: auto  # 較低的目標熵 ############
normalize_input: True  # 是否對觀察值進行標準化
normalize_value: True  # 是否對價值函數進行標準化
tau: 0.005  # 目標網路更新的平滑係數
# ent_coef: 0.01 # 熵正則化係數，設為 "auto" 以自動調整 ############
# target_entropy: -action_dim  # 目標熵值，默認為自動計算
buffer_size: 1000000  # 經驗回放緩衝區大小
device: "cuda:0"
clip_obs: 10
# use_sde : "True"
# use_sde_at_warmup : "True"
# policy_kwargs: "dict(
#                   log_std_init=-5,
#                   net_arch=dict(pi=[128, 128], qf=[128, 128]),
#                   activation_fn=nn.ReLU,
#                   use_sde=True,
#                 )" ############
# 添加動作噪音控制

# --------------------PPO--------------------
# policy: 'MlpPolicy'
# seed: 42
# n_timesteps: !!float 5e7
# batch_size: 256
# n_steps: 512
# gamma: 0.99
# learning_rate: !!float 2.5e-4
# ent_coef: 0.0
# clip_range: 0.2
# n_epochs: 10
# gae_lambda: 0.95
# max_grad_norm: 1.0
# vf_coef: 0.5
# device: "cuda:0"
# policy_kwargs: "dict(
#                   log_std_init=-1,
#                   ortho_init=False,
#                   activation_fn=nn.ReLU,
#                   net_arch=dict(pi=[256, 256], vf=[256, 256])
#                 )"